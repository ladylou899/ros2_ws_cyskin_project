function [softmax1000, state] = HandsNetFcn(imageinput, params, varargin)
%HANDSNETFCN Function implementing an imported ONNX network.
%
% THIS FILE WAS AUTO-GENERATED BY importONNXFunction.
% ONNX Operator Set Version: 8
%
% Variable names in this function are taken from the original ONNX file.
%
% [SOFTMAX1000] = HandsNetFcn(IMAGEINPUT, PARAMS)
%			- Evaluates the imported ONNX network HANDSNETFCN with input(s)
%			IMAGEINPUT and the imported network parameters in PARAMS. Returns
%			network output(s) in SOFTMAX1000.
%
% [SOFTMAX1000, STATE] = HandsNetFcn(IMAGEINPUT, PARAMS)
%			- Additionally returns state variables in STATE. When training,
%			use this form and set TRAINING to true.
%
% [__] = HandsNetFcn(IMAGEINPUT, PARAMS, 'NAME1', VAL1, 'NAME2', VAL2, ...)
%			- Specifies additional name-value pairs described below:
%
% 'Training'
% 			Boolean indicating whether the network is being evaluated for
%			prediction or training. If TRAINING is true, state variables
%			will be updated.
%
% 'InputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			 between the dimensions of the input data and the dimensions of
%			the ONNX model input. For example, the permutation from HWCN
%			(MATLAB standard) to NCHW (ONNX standard) uses the vector
%			[4 3 1 2]. See the documentation for IMPORTONNXFUNCTION for
%			more information about automatic permutation.
%
%			'none' - Input(s) are passed in the ONNX model format. See 'Inputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between input data dimensions and the expected
%			ONNX input dimensions.%
%			cell array - If the network has multiple inputs, each cell
%			contains 'auto', 'none', or a numeric vector.
%
% 'OutputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			between the dimensions of the output and a conventional MATLAB
%			dimension ordering. For example, the permutation from NC (ONNX
%			standard) to CN (MATLAB standard) uses the vector [2 1]. See
%			the documentation for IMPORTONNXFUNCTION for more information
%			about automatic permutation.
%
%			'none' - Return output(s) as given by the ONNX model. See 'Outputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between the ONNX output dimensions and the
%			desired output dimensions.%
%			cell array - If the network has multiple outputs, each cell
%			contains 'auto', 'none' or a numeric vector.
%
% Inputs:
% -------
% IMAGEINPUT
%			- Input(s) to the ONNX network.
%			  The input size(s) expected by the ONNX file are:
%				  IMAGEINPUT:		[1, 1, 68, 100]				Type: FLOAT
%			  By default, the function will try to permute the input(s)
%			  into this dimension ordering. If the default is incorrect,
%			  use the 'InputDataPermutation' argument to control the
%			  permutation.
%
%
% PARAMS	- Network parameters returned by 'importONNXFunction'.
%
%
% Outputs:
% --------
% SOFTMAX1000
%			- Output(s) of the ONNX network.
%			  Without permutation, the size(s) of the outputs are:
%				  SOFTMAX1000:		[1, 2]				Type: FLOAT
%			  By default, the function will try to permute the output(s)
%			  from this dimension ordering into a conventional MATLAB
%			  ordering. If the default is incorrect, use the
%			  'OutputDataPermutation' argument to control the permutation.
%
% STATE		- (Optional) State variables. When TRAINING is true, these will
% 			  have been updated from the original values in PARAMS.State.
%
%
%  See also importONNXFunction

% Preprocess the input data and arguments:
[imageinput, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(imageinput, params, varargin{:});
% Put all variables into a single struct to implement dynamic scoping:
[Vars, NumDims] = packageVariables(params, {'imageinput'}, {imageinput}, [4]);
% Call the top-level graph function:
[softmax1000, softmax1000NumDims, state] = NetworkGraph1001(imageinput, NumDims.imageinput, Vars, NumDims, Training, params.State);
% Postprocess the output data
[softmax1000] = postprocessOutput(softmax1000, outputDataPerms, anyDlarrayInputs, Training, varargin{:});
end

function [softmax1000, softmax1000NumDims1036, state] = NetworkGraph1001(imageinput, imageinputNumDims1035, Vars, NumDims, Training, state)
% Function implementing the graph 'NetworkGraph1001'
% Update Vars and NumDims from the graph's formal input parameters. Note that state variables are already in Vars.
Vars.imageinput = imageinput;
NumDims.imageinput = imageinputNumDims1035;

% Execute the operators:
% Sub:
Vars.imageinput_Sub = Vars.imageinput - Vars.imageinput_Mean;
NumDims.imageinput_Sub = max(NumDims.imageinput, NumDims.imageinput_Mean);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.conv_1] = prepareConvArgs(Vars.conv_1_W, Vars.conv_1_B, Vars.ConvStride1002, Vars.ConvDilationFactor1003, Vars.ConvPadding1004, 1, NumDims.imageinput_Sub, NumDims.conv_1_W);
Vars.conv_1 = dlconv(Vars.imageinput_Sub, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.batchnorm_1, NumDims.batchnorm_1_mean, NumDims.batchnorm_1_var] = prepareBatchNormalizationArgs(Vars.batchnorm_1_B, Vars.batchnorm_1_scale, Vars.batchnorm_1_mean, Vars.batchnorm_1_var, NumDims.conv_1, NumDims.batchnorm_1_mean, NumDims.batchnorm_1_var);
if Training
    [Vars.batchnorm_1, dsmean, dsvar] = batchnorm(Vars.conv_1, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.batchnorm_1_mean = dlarray(dsmean);
    Vars.batchnorm_1_var = dlarray(dsvar);
else
    Vars.batchnorm_1 = batchnorm(Vars.conv_1, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.batchnorm_1_mean = Vars.batchnorm_1_mean;
state.batchnorm_1_var = Vars.batchnorm_1_var;

% Relu:
Vars.relu_1 = relu(Vars.batchnorm_1);
NumDims.relu_1 = NumDims.batchnorm_1;

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.maxpool_1] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1005, Vars.MaxPoolStride1006, Vars.MaxPoolPadding1007, NumDims.relu_1);
Vars.maxpool_1 = maxpool(Vars.relu_1, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Dropout:
[Vars.dropout_1, NumDims.dropout_1] = onnxDropout7(Vars.maxpool_1, 0.100000, Training, NumDims.maxpool_1);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.conv_2] = prepareConvArgs(Vars.conv_2_W, Vars.conv_2_B, Vars.ConvStride1008, Vars.ConvDilationFactor1009, Vars.ConvPadding1010, 1, NumDims.dropout_1, NumDims.conv_2_W);
Vars.conv_2 = dlconv(Vars.dropout_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.batchnorm_2, NumDims.batchnorm_2_mean, NumDims.batchnorm_2_var] = prepareBatchNormalizationArgs(Vars.batchnorm_2_B, Vars.batchnorm_2_scale, Vars.batchnorm_2_mean, Vars.batchnorm_2_var, NumDims.conv_2, NumDims.batchnorm_2_mean, NumDims.batchnorm_2_var);
if Training
    [Vars.batchnorm_2, dsmean, dsvar] = batchnorm(Vars.conv_2, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.batchnorm_2_mean = dlarray(dsmean);
    Vars.batchnorm_2_var = dlarray(dsvar);
else
    Vars.batchnorm_2 = batchnorm(Vars.conv_2, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.batchnorm_2_mean = Vars.batchnorm_2_mean;
state.batchnorm_2_var = Vars.batchnorm_2_var;

% Relu:
Vars.relu_2 = relu(Vars.batchnorm_2);
NumDims.relu_2 = NumDims.batchnorm_2;

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.maxpool_2] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1011, Vars.MaxPoolStride1012, Vars.MaxPoolPadding1013, NumDims.relu_2);
Vars.maxpool_2 = maxpool(Vars.relu_2, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Dropout:
[Vars.dropout_2, NumDims.dropout_2] = onnxDropout7(Vars.maxpool_2, 0.200000, Training, NumDims.maxpool_2);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.conv_3] = prepareConvArgs(Vars.conv_3_W, Vars.conv_3_B, Vars.ConvStride1014, Vars.ConvDilationFactor1015, Vars.ConvPadding1016, 1, NumDims.dropout_2, NumDims.conv_3_W);
Vars.conv_3 = dlconv(Vars.dropout_2, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.batchnorm_3, NumDims.batchnorm_3_mean, NumDims.batchnorm_3_var] = prepareBatchNormalizationArgs(Vars.batchnorm_3_B, Vars.batchnorm_3_scale, Vars.batchnorm_3_mean, Vars.batchnorm_3_var, NumDims.conv_3, NumDims.batchnorm_3_mean, NumDims.batchnorm_3_var);
if Training
    [Vars.batchnorm_3, dsmean, dsvar] = batchnorm(Vars.conv_3, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.batchnorm_3_mean = dlarray(dsmean);
    Vars.batchnorm_3_var = dlarray(dsvar);
else
    Vars.batchnorm_3 = batchnorm(Vars.conv_3, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.batchnorm_3_mean = Vars.batchnorm_3_mean;
state.batchnorm_3_var = Vars.batchnorm_3_var;

% Relu:
Vars.relu_3 = relu(Vars.batchnorm_3);
NumDims.relu_3 = NumDims.batchnorm_3;

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.maxpool_3] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1017, Vars.MaxPoolStride1018, Vars.MaxPoolPadding1019, NumDims.relu_3);
Vars.maxpool_3 = maxpool(Vars.relu_3, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Dropout:
[Vars.dropout_3, NumDims.dropout_3] = onnxDropout7(Vars.maxpool_3, 0.300000, Training, NumDims.maxpool_3);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.conv_4] = prepareConvArgs(Vars.conv_4_W, Vars.conv_4_B, Vars.ConvStride1020, Vars.ConvDilationFactor1021, Vars.ConvPadding1022, 1, NumDims.dropout_3, NumDims.conv_4_W);
Vars.conv_4 = dlconv(Vars.dropout_3, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.batchnorm_4, NumDims.batchnorm_4_mean, NumDims.batchnorm_4_var] = prepareBatchNormalizationArgs(Vars.batchnorm_4_B, Vars.batchnorm_4_scale, Vars.batchnorm_4_mean, Vars.batchnorm_4_var, NumDims.conv_4, NumDims.batchnorm_4_mean, NumDims.batchnorm_4_var);
if Training
    [Vars.batchnorm_4, dsmean, dsvar] = batchnorm(Vars.conv_4, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.batchnorm_4_mean = dlarray(dsmean);
    Vars.batchnorm_4_var = dlarray(dsvar);
else
    Vars.batchnorm_4 = batchnorm(Vars.conv_4, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.batchnorm_4_mean = Vars.batchnorm_4_mean;
state.batchnorm_4_var = Vars.batchnorm_4_var;

% Relu:
Vars.relu_4 = relu(Vars.batchnorm_4);
NumDims.relu_4 = NumDims.batchnorm_4;

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.maxpool_4] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1023, Vars.MaxPoolStride1024, Vars.MaxPoolPadding1025, NumDims.relu_4);
Vars.maxpool_4 = maxpool(Vars.relu_4, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Dropout:
[Vars.dropout_4, NumDims.dropout_4] = onnxDropout7(Vars.maxpool_4, 0.400000, Training, NumDims.maxpool_4);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.fc_1] = prepareConvArgs(Vars.fc_1_W, Vars.fc_1_B, Vars.ConvStride1026, Vars.ConvDilationFactor1027, Vars.ConvPadding1028, 1, NumDims.dropout_4, NumDims.fc_1_W);
Vars.fc_1 = dlconv(Vars.dropout_4, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Dropout:
[Vars.dropout_5, NumDims.dropout_5] = onnxDropout7(Vars.fc_1, 0.600000, Training, NumDims.fc_1);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.fc_2] = prepareConvArgs(Vars.fc_2_W, Vars.fc_2_B, Vars.ConvStride1029, Vars.ConvDilationFactor1030, Vars.ConvPadding1031, 1, NumDims.dropout_5, NumDims.fc_2_W);
Vars.fc_2 = dlconv(Vars.dropout_5, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Dropout:
[Vars.dropout_6, NumDims.dropout_6] = onnxDropout7(Vars.fc_2, 0.800000, Training, NumDims.fc_2);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.fc_3] = prepareConvArgs(Vars.fc_3_W, Vars.fc_3_B, Vars.ConvStride1032, Vars.ConvDilationFactor1033, Vars.ConvPadding1034, 1, NumDims.dropout_6, NumDims.fc_3_W);
Vars.fc_3 = dlconv(Vars.dropout_6, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Flatten:
[dim1, dim2, NumDims.softmax_Flatten] = prepareFlattenArgs(Vars.fc_3, 1, NumDims.fc_3);
Vars.softmax_Flatten = reshape(Vars.fc_3, dim1, dim2);

% Softmax:
[dim1, dim2, origSize, NumDims.softmax1000] = prepareSoftmaxArgs(Vars.softmax_Flatten, 1, NumDims.softmax_Flatten);
Vars.softmax1000 = reshape(Vars.softmax_Flatten, dim1, dim2);
Vars.softmax1000 = softmax(Vars.softmax1000, 'DataFormat', 'CB');
Vars.softmax1000 = reshape(Vars.softmax1000, origSize);

% Set graph output arguments from Vars and NumDims:
softmax1000 = Vars.softmax1000;
softmax1000NumDims1036 = NumDims.softmax1000;
% Set output state from Vars:
state = updateStruct(state, Vars);
end

function [inputDataPerms, outputDataPerms, Training] = parseInputs(imageinput, numDataOutputs, params, varargin)
% Function to validate inputs to HandsNetFcn:
p = inputParser;
isValidArrayInput = @(x)isnumeric(x) || isstring(x);
isValidONNXParameters = @(x)isa(x, 'ONNXParameters');
addRequired(p, 'imageinput', isValidArrayInput);
addRequired(p, 'params', isValidONNXParameters);
addParameter(p, 'InputDataPermutation', 'auto');
addParameter(p, 'OutputDataPermutation', 'auto');
addParameter(p, 'Training', false);
parse(p, imageinput, params, varargin{:});
inputDataPerms = p.Results.InputDataPermutation;
outputDataPerms = p.Results.OutputDataPermutation;
Training = p.Results.Training;
if isnumeric(inputDataPerms)
    inputDataPerms = {inputDataPerms};
end
if isstring(inputDataPerms) && isscalar(inputDataPerms) || ischar(inputDataPerms)
    inputDataPerms = repmat({inputDataPerms},1,1);
end
if isnumeric(outputDataPerms)
    outputDataPerms = {outputDataPerms};
end
if isstring(outputDataPerms) && isscalar(outputDataPerms) || ischar(outputDataPerms)
    outputDataPerms = repmat({outputDataPerms},1,numDataOutputs);
end
end

function [imageinput, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(imageinput, params, varargin)
% Parse input arguments
[inputDataPerms, outputDataPerms, Training] = parseInputs(imageinput, 1, params, varargin{:});
anyDlarrayInputs = any(cellfun(@(x)isa(x, 'dlarray'), {imageinput}));
% Make the input variables into unlabelled dlarrays:
imageinput = makeUnlabeledDlarray(imageinput);
% Permute inputs if requested:
imageinput = permuteInputVar(imageinput, inputDataPerms{1}, 4);
% Check input size(s):
checkInputSize(size(imageinput), {1 1 68 100}, "imageinput");
end

function [softmax1000] = postprocessOutput(softmax1000, outputDataPerms, anyDlarrayInputs, Training, varargin)
% Set output type:
if ~anyDlarrayInputs && ~Training
    if isdlarray(softmax1000)
        softmax1000 = extractdata(softmax1000);
    end
end
% Permute outputs if requested:
softmax1000 = permuteOutputVar(softmax1000, outputDataPerms{1}, 2);
end


%% dlarray functions implementing ONNX operators:

function [Y, numDimsY, Mask, numDimsMask] = onnxDropout7(X, ratio, Training, numDimsX)
% Implements the ONNX Dropout-7 operator.
if Training
    Scale = 1/(1 - ratio);
    Mask = rand(size(X),'like',X) >= ratio;
    Y = Scale .* X .* Mask;
else
    Y = X;
    Mask = ones(size(X),'like',X);
end
numDimsY = numDimsX;
numDimsMask = numDimsX;
end

function [offset, scale, datasetMean, datasetVariance, dataFormat, numDimsY, numDimsDatasetMean, numDimsDatasetVariance] = prepareBatchNormalizationArgs(...
    offset, scale, datasetMean, datasetVariance, numDimsX, numDimsDatasetMean, numDimsDatasetVariance)
% Prepares arguments for implementing the ONNX BatchNormalization operator
offset = dlarray(offset,'C');
scale = dlarray(scale,'C');
datasetMean = extractdata(datasetMean);
datasetVariance = extractdata(datasetVariance);
datasetVariance(datasetVariance <= 0) = realmin('single');  % Set nonpositive variance components to a value below eps('single')
dataFormat = [repmat('S', 1, numDimsX-2), 'CB'];
numDimsY = numDimsX;
end

function [weights, bias, stride, dilationFactor, padding, dataFormat, numDimsY] = prepareConvArgs(...
    weights, bias, stride, dilationFactor, padding, numWtGroups, numDimsX, numDimsW)
% Prepares arguments for implementing the ONNX Conv operator

% Weights: The ONNX weight dim is Fcxyz..., where c=C/G, G is numGroups,
% and xyz... are spatial dimensions. DLT "weights" here is the flip of
% that, or ...zyxcF. dlconv requires ...zyxcfG, where f=F/G. So reshape to
% split the last dimension.
sizeW    = size(weights, 1:numDimsW);
F        = sizeW(end);
newWSize = [sizeW(1:numDimsW-1), F/numWtGroups, numWtGroups];
weights  = reshape(weights, newWSize);
% bias
if isempty(bias)
    bias = 0;
end
bias = dlarray(bias(:),'CU');
% Derive missing default attributes from weight tensor
numSpatialDims = numDimsW-2;
if isempty(padding)
    padding = zeros(1, 2*numSpatialDims);
end
if isempty(stride)
    stride = ones(1,numSpatialDims);
end
if isempty(dilationFactor)
    dilationFactor = ones(1,numSpatialDims);
end
% Make the attributes non-dlarrays:
if isa(stride, 'dlarray')
    stride = extractdata(stride);
end
if isa(dilationFactor, 'dlarray')
    dilationFactor = extractdata(dilationFactor);
end
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
% Make the attributes double row vectors, and flip their dimension ordering
% to reverse-onnx:
stride = fliplr(double(stride(:)'));
dilationFactor = fliplr(double(dilationFactor(:)'));
if isnumeric(padding)       % padding can be "same"
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the lrflip and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
% Set dataformat and numdims
dataFormat = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY = numDimsX;
end

function [dim1, dim2, numDimsY] = prepareFlattenArgs(X, ONNXAxis, numDimsX)
% Prepares arguments for implementing the ONNX Flatten operator

% ONNXAxis is the number of dimensions that go on the left in ONNX, so here
% it is the number of dimensions that go on the right.
if ONNXAxis < 0
    ONNXAxis = ONNXAxis + numDimsX;
end
if ONNXAxis == 0
    dim2 = 1;
else
    dim2 = prod(size(X, numDimsX+1-ONNXAxis:numDimsX));     % numel on the right
end
dim1 = numel(X)/dim2;                                   % numel on the left
numDimsY = 2;
end

function [poolsize, stride, padding, dataFormat, numDimsY, numDimsIndices] = prepareMaxPool8Args(poolsize, stride, padding, numDimsX)
% Prepares arguments for implementing the ONNX MaxPool-8 operator
poolsize    = fliplr(extractdata(poolsize(:)'));
stride      = fliplr(extractdata(stride(:)'));
% padding
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
if isnumeric(padding)
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the lrflip and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
dataFormat  = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY    = numDimsX;
numDimsIndices = numDimsX;                      % New in opset 8
end

function [dim1, dim2, origSize, numDimsX] = prepareSoftmaxArgs(X, ONNXAxis, numDimsX)
% Prepares arguments for implementing the ONNX Softmax operator
if ONNXAxis<0
    ONNXAxis = ONNXAxis + numDimsX;
end
dim2     = prod(size(X, numDimsX+1-ONNXAxis:numDimsX));   % numel on the right
dim1     = numel(X)/dim2;                                 % numel on the left
origSize = size(X);
end

%% Utility functions:

function s = appendStructs(varargin)
% s = appendStructs(s1, s2,...). Assign all fields in s1, s2,... into s.
if isempty(varargin)
    s = struct;
else
    s = varargin{1};
    for i = 2:numel(varargin)
        fromstr = varargin{i};
        fs = fieldnames(fromstr);
        for j = 1:numel(fs)
            s.(fs{j}) = fromstr.(fs{j});
        end
    end
end
end

function checkInputSize(inputShape, expectedShape, inputName)

if numel(expectedShape)==0
    % The input is a scalar
    if ~isequal(inputShape, [1 1])
        inputSizeStr = makeSizeString(inputShape);
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, "[1,1]", inputSizeStr));
    end
elseif numel(expectedShape)==1
    % The input is a vector
    if ~shapeIsColumnVector(inputShape) || ~iSizesMatch({inputShape(1)}, expectedShape)
        expectedShape{2} = 1;
        expectedSizeStr = makeSizeString(expectedShape);
        inputSizeStr = makeSizeString(inputShape);
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
    end
else
    % The input has 2 dimensions or more
    
    % The input dimensions have been reversed; flip them back to compare to the
    % expected ONNX shape.
    inputShape = fliplr(inputShape);
    
    % If the expected shape has fewer dims than the input shape, error.
    if numel(expectedShape) < numel(inputShape)
        expectedSizeStr = strjoin(["[", strjoin(string(expectedShape), ","), "]"], "");
        error(message('nnet_cnn_onnx:onnx:InputHasGreaterNDims', inputName, expectedSizeStr));
    end
    
    % Prepad the input shape with trailing ones up to the number of elements in
    % expectedShape
    inputShape = num2cell([ones(1, numel(expectedShape) - length(inputShape)) inputShape]);
    
    % Find the number of variable size dimensions in the expected shape
    numVariableInputs = sum(cellfun(@(x) isa(x, 'char') || isa(x, 'string'), expectedShape));
    
    % Find the number of input dimensions that are not in the expected shape
    % and cannot be represented by a variable dimension
    nonMatchingInputDims = setdiff(string(inputShape), string(expectedShape));
    numNonMatchingInputDims  = numel(nonMatchingInputDims) - numVariableInputs;
    
    expectedSizeStr = makeSizeString(expectedShape);
    inputSizeStr = makeSizeString(inputShape);
    if numNonMatchingInputDims == 0 && ~iSizesMatch(inputShape, expectedShape)
        % The actual and expected input dimensions match, but in
        % a different order. The input needs to be permuted.
        error(message('nnet_cnn_onnx:onnx:InputNeedsPermute',inputName, expectedSizeStr, inputSizeStr));
    elseif numNonMatchingInputDims > 0
        % The actual and expected input sizes do not match.
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
    end
end
end

function doesMatch = iSizesMatch(inputShape, expectedShape)
% Check whether the input and expected shapes match, in order.
% Size elements match if (1) the elements are equal, or (2) the expected
% size element is a variable (represented by a character vector or string)
doesMatch = true;
for i=1:numel(inputShape)
    if ~(isequal(inputShape{i},expectedShape{i}) || ischar(expectedShape{i}) || isstring(expectedShape{i}))
        doesMatch = false;
        return
    end
end
end

function sizeStr = makeSizeString(shape)
sizeStr = strjoin(["[", strjoin(string(shape), ","), "]"], "");
end

function isVec = shapeIsColumnVector(shape)
if numel(shape) == 2 && shape(2) == 1
    isVec = true;
else
    isVec = false;
end
end
function X = makeUnlabeledDlarray(X)
% Make numeric X into an unlabelled dlarray
if isa(X, 'dlarray')
    X = stripdims(X);
elseif isnumeric(X)
    if isinteger(X)
        % Make ints double so they can combine with anything without
        % reducing precision
        X = double(X);
    end
    X = dlarray(X);
end
end

function [Vars, NumDims] = packageVariables(params, inputNames, inputValues, inputNumDims)
% inputNames, inputValues are cell arrays. inputRanks is a numeric vector.
Vars = appendStructs(params.Learnables, params.Nonlearnables, params.State);
NumDims = params.NumDimensions;
% Add graph inputs
for i = 1:numel(inputNames)
    Vars.(inputNames{i}) = inputValues{i};
    NumDims.(inputNames{i}) = inputNumDims(i);
end
end

function X = permuteInputVar(X, userDataPerm, onnxNDims)
% Returns reverse-ONNX ordering
if onnxNDims == 0
    return;
elseif onnxNDims == 1 && isvector(X)
    X = X(:);
    return;
elseif isnumeric(userDataPerm)
    % Permute into reverse ONNX ordering
    if numel(userDataPerm) ~= onnxNDims
        error(message('nnet_cnn_onnx:onnx:InputPermutationSize', numel(userDataPerm), onnxNDims));
    end
    perm = fliplr(userDataPerm);
elseif isequal(userDataPerm, 'auto') && onnxNDims == 4
    % Permute MATLAB HWCN to reverse onnx (WHCN)
    perm = [2 1 3 4];
elseif isequal(userDataPerm, 'as-is')
    % Do not permute the input
    perm = 1:ndims(X);
else
    % userDataPerm is either 'none' or 'auto' with no default, which means
    % it's already in onnx ordering, so just make it reverse onnx
    perm = max(2,onnxNDims):-1:1;
end
X = permute(X, perm);
end

function Y = permuteOutputVar(Y, userDataPerm, onnxNDims)
switch onnxNDims
    case 0
        perm = [];
    case 1
        if isnumeric(userDataPerm)
            % Use the user's permutation because Y is a column vector which
            % already matches ONNX.
            perm = userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            % Treat the 1D onnx vector as a 2D column and transpose it
            perm = [2 1];
        else
            % userDataPerm is 'none'. Leave Y alone because it already
            % matches onnx.
            perm = [];
        end
    otherwise
        % ndims >= 2
        if isnumeric(userDataPerm)
            % Use the inverse of the user's permutation. This is not just the
            % flip of the permutation vector.
            perm = onnxNDims + 1 - userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            if onnxNDims == 2
                % Permute reverse ONNX CN to DLT CN (do nothing)
                perm = [];
            elseif onnxNDims == 4
                % Permute reverse onnx (WHCN) to MATLAB HWCN
                perm = [2 1 3 4];
            else
                % User wants the output in ONNX ordering, so just reverse it from
                % reverse onnx
                perm = onnxNDims:-1:1;
            end
        elseif isequal(userDataPerm, 'as-is')
            % Do not permute the input
            perm = 1:ndims(Y);
        else
            % userDataPerm is 'none', so just make it reverse onnx
            perm = onnxNDims:-1:1;
        end
end
if ~isempty(perm)
    Y = permute(Y, perm);
end
end

function s = updateStruct(s, t)
% Set all existing fields in s from fields in t, ignoring extra fields in t.
for name = transpose(fieldnames(s))
    s.(name{1}) = t.(name{1});
end
end
